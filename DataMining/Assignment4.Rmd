---
title: "CS 422 Assignment 4"
author: "Nam Gyu Lee"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    toc_float: yes
---

```{r, setup, include=FALSE}
# The setup chunk, set your working directory here and do other global 
# tasks like loading libraries.  See  
# https://bookdown.org/yihui/rmarkdown-cookbook/working-directory.html for
# more information.
knitr::opts_knit$set(root.dir = '/Users/namgyulee/IIT/Fall 2023/CS 422/')
```

## 2. Practicum problems
### 2.1 Regression
```{r}
df1 <- read.csv("/Users/namgyulee/IIT/Fall 2023/CS 422/CS422_Assignment3_NamGyu_Lee/nba.csv")
df1
```


#### (a)
```{r}
library(olsrr)
set.seed(1122)
index <- sample(1:nrow(df1), 250)
train <- df1[index, ]
test <- df1[-index, ]

model <- lm(PTS ~ FG + FGA + MIN + FT + FTA, data = train)
ols_step_both_p(model)
```

##### i)
There are not the parameters that are statistically significant at an \alpha of at least 0.05.

##### ii)
FG is the most important parameter for prediction. Because its standard beta is the largest.

##### iii)
MIN is the least important parameter for prediction. Because its standard beta is the smallest.

#### b)
```{r}
train_model <- subset(train, select = c(FG, FGA, MIN, FT, FTA, PTS))
test_model <- subset(test, select = c(FG, FGA, MIN, FT, FTA, PTS))

predictions <- predict(model, newdata = test_model, interval = "prediction", level = 0.95)

lwr <- predictions[, "lwr"]
upr <- predictions[, "upr"]
fit <- predictions[, "fit"]

prediction_df <- data.frame(fit = round(fit, digits = 3), lwr = round(lwr, digits = 3), upr = round(upr, digits = 3), PTS = test_model$PTS)

matches <- sum(prediction_df$PTS >= prediction_df$lwr & prediction_df$PTS <= prediction_df$upr)

print(prediction_df)
matches <- sum(test_model$PTS >= lwr & test_model$PTS <= upr)
cat("Number of predictions that are in the prediction interval:", matches)

```

### c)
```{r}
n <- length(prediction_df$fit)
p <- 5
rss <- sum((prediction_df$fit - prediction_df$PTS)^2)
rse <- sqrt(rss/ (n-p-1))

cat("RSS:", rss, "\n")
cat("RSE:", rse, "\n")
```


### d)
```{r}
prediction_df$residuals <- prediction_df$PTS - prediction_df$fit
plot(prediction_df$residuals, xlab = "Residuals")
abline(0, 0)
```

With the plot could not determine it follows which distribution.

### e)
```{r}
hist(prediction_df$residuals)
```

Yes, it follow the Gaussian distribution.

## 2.2 Decision Trees
```{r}
df2 <- read.csv("/Users/namgyulee/IIT/Fall 2023/CS 422/titanic-new.csv")
df2

set.seed(1122)
index2 <- sample(1:nrow(df2), nrow(df2) * 0.8)
train2 <- df2[index2, ]
test2 <- df2[-index2, ]
```

### a)
```{r}
summary(df2)
cor_mat <- cor(df2[, c("age", "sibsp", "pclass", "survived")])
print(cor_mat)

select_predictors <- c("age", "sex", "pclass", "sibsp")
print(paste("Selected Predictors: ", select_predictors))
```

### b)
```{r}
df2$survived <- factor(df2$survived, levels = c(0, 1), labels = c("Perished", "Survivede"))
num_survival <- table(df2$survived)
print(num_survival)
```

### c)
The dataset is not class-balanced, because the number of perished passengers is more than 50% of the number of surviving passengers, which is 809, higher than 750. The unbalanced dataset could lead biased model and difficult to evaluate the model.

### d)
```{r}
library(rpart)

tree <- rpart(survived ~ age + sex + pclass + sibsp, data = train2, method = "class")

summary(tree)
```

### e)
```{r}
library(rpart)
library(rpart.plot)

model2 <- lm(survived ~ age + sex + pclass + sibsp, data = train2)
ols_step_both_p(model2)
```

The top 3 important variables are "sibsp", "age", and "pclass", because they are the top 3 of higher standard beta.

```{r}
library(rpart)
library(rpart.plot)

rpart.plot(tree, extra = 101, fallen.leaves=T, type=4, main = "Titanic Survival Model")
print(tree)
```

### f)
With my model, the default class of the root node is "Class 0".

### g)
#### i)
The default class of node number 2, corresponds to the condition "Sex = Male", is "Class 0"

#### ii)
There are 121 misclassified in this node.

### h)
#### i)
The default class of the node number 3, corresponds to the condition "Sex = Female", is "Class 1".

#### ii)
There are 110 misclassified in this node.